import numpy as np
import torch

from .trainer import Trainer
from .utils.general import make_save_path


def train(csv_file,
          net_name,
          number_nets_to_train,
          base_learning_rate,
          freeze_trained_weights,
          new_learn_rate_layers,
          new_layer_learning_rate,
          epochs_list,
          batch_size,
          random_seed,
          save_path,
          loss_func='CE',
          triplet_loss_margin=0.5,
          squared_dist=False,
          use_val=True,
          val_epoch=None,
          summary_step=None,
          patience=None,
          checkpoint_epoch=None,
          save_acc_by_set_size_by_epoch=False,
          num_workers=4):
    """train convolutional neural networks to perform visual search task.

    Parameters
    ----------
    csv_file : str
        name of .csv file containing prepared data sets.
        Generated by searchnets.data.split function from a csv created by the searchstims library.
    net_name : str
        name of convolutional neural net architecture to train.
        One of {'alexnet', 'VGG16'}
    number_nets_to_train : int
        number of training "replicates"
    base_learning_rate : float
        Applied to layers with weights loaded from training the
        architecture on ImageNet. Should be a very small number
        so the trained weights don't change much.
    freeze_trained_weights : bool
        if True, freeze weights in any layer not in "new_learn_rate_layers".
        These are the layers that have weights pre-trained on ImageNet.
        Default is False. Done by simply not applying gradients to these weights,
        i.e. this will ignore a base_learning_rate if you set it to something besides zero.
    new_learn_rate_layers : list
        of layer names whose weights will be initialized randomly
        and then trained with the 'new_layer_learning_rate'.
    new_layer_learning_rate : float
        Applied to `new_learn_rate_layers'. Should be larger than
        `base_learning_rate` but still smaller than the usual
        learning rate for a deep net trained with SGD,
        e.g. 0.001 instead of 0.01
    epochs_list : list
        of training epochs. Replicates will be trained for each
        value in this list. Can also just be one value, but a list
        is useful if you want to test whether effects depend on
        number of training epochs.
    batch_size : int
        number of samples in a batch of training data
    random_seed : int
        to seed random number generator
    save_path : str
        path to directory where checkpoints and train models were saved
    loss_func : str
        type of loss function to use. One of {'CE', 'InvDPrime', 'triplet'}. Default is 'CE',
        the standard cross-entropy loss. 'InvDPrime' is inverse D prime. 'triplet' is triplet loss
        used in face recognition and biometric applications.
    triplet_loss_margin : float
        Minimum margin between clusters, parameter in triplet loss function. Default is 0.5.
    squared_dist : bool
        if True, when computing similarity of embeddings (e.g. for triplet loss), use pairwise squared
        distance, i.e. Euclidean distance.
    save_acc_by_set_size_by_epoch : bool
        if True, compute accuracy on training set for each epoch separately
        for each unique set size in the visual search stimuli. These values
        are saved in a matrix where rows are epochs and columns are set sizes.
        Useful for seeing whether accuracy converges for each individual
        set size. Default is False.
    use_val : bool
        if True, use validation set.
    val_epoch : int
        if not None, accuracy on validation set will be measured every `val_epoch` epochs. Default is None.
    summary_step : int
        Step on which to write summaries to file. Each minibatch is counted as one step, and steps are counted across
        epochs. Default is None.
    patience : int
        if not None, training will stop if accuracy on validation set has not improved in `patience` steps
    num_workers : int
        number of workers used by torch.DataLoaders. Default is 4.

    Returns
    -------
    None
    """
    if use_val and val_epoch is None or val_epoch < 1 or type(val_epoch) != int:
        raise ValueError(
            f'invalid value for val_epoch: {val_epoch}. Validation epoch must be positive integer'
        )

    if use_val is False and patience is not None:
        raise ValueError('patience argument only works with a validation set')

    if patience is not None:
        if type(val_epoch) != int or patience < 1:
            raise TypeError('patience must be a positive integer')

    if type(epochs_list) is int:
        epochs_list = [epochs_list]
    elif type(epochs_list) is list:
        pass
    else:
        raise TypeError("'EPOCHS' option in 'TRAIN' section of config.ini file parsed "
                        f"as invalid type: {type(epochs_list)}")

    if random_seed:
        np.random.seed(random_seed)  # for shuffling in batch_generator
        torch.manual_seed(random_seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

    if torch.cuda.is_available():
        device = torch.device('cuda')
    else:
        device = torch.device('cpu')

    for epochs in epochs_list:
        print(f'training {net_name} model for {epochs} epochs')
        for net_number in range(1, number_nets_to_train + 1):
            save_path_this_net = make_save_path(save_path, net_name, net_number, epochs)
            trainer = Trainer(net_name=net_name,
                              new_learn_rate_layers=new_learn_rate_layers,
                              csv_file=csv_file,
                              save_path=save_path_this_net,
                              loss_func=loss_func,
                              save_acc_by_set_size_by_epoch=save_acc_by_set_size_by_epoch,
                              freeze_trained_weights=freeze_trained_weights,
                              base_learning_rate=base_learning_rate,
                              new_layer_learning_rate=new_layer_learning_rate,
                              batch_size=batch_size,
                              epochs=epochs,
                              val_epoch=val_epoch,
                              use_val=use_val,
                              patience=patience,
                              checkpoint_epoch=checkpoint_epoch,
                              summary_step=summary_step,
                              device=device,
                              num_workers=num_workers)
            trainer.train()
