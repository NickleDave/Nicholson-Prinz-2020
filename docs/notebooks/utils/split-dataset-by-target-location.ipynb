{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path('/home/bart/Documents/repos/L2M/visual-search-nets/data')\n",
    "\n",
    "data_gz_fname = DATA_ROOT.joinpath('data_prepd_for_nets/alexnet_train_2_v_5_data.gz')\n",
    "json_fname = DATA_ROOT.joinpath('visual_search_stimuli/alexnet_train_2_v_5/alexnet_train_2_v_5.json')\n",
    "stim_abbrev = '2_v_5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gz = joblib.load(data_gz_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_fname) as fp:\n",
    "    stim_meta_dict = json.load(fp)\n",
    "\n",
    "stim_meta_list = []\n",
    "stim_meta_dict = stim_meta_dict[stim_abbrev]\n",
    "for set_size, stim_meta_this_set_size in stim_meta_dict.items():\n",
    "    # *** only using present because we only care about splitting target present condition up ***\n",
    "    stim_meta_list.extend(stim_meta_this_set_size['present'])\n",
    "\n",
    "fname_grid_map = {}\n",
    "for meta_d in stim_meta_list:\n",
    "    stim_fname_meta = Path(meta_d['filename']).name\n",
    "    char_grid = np.asarray(meta_d['grid_as_char'])\n",
    "    fname_grid_map[stim_fname_meta] = char_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_SHAPE = (5, 5)\n",
    "train_mask = np.zeros(GRID_SHAPE).astype(np.int32)\n",
    "train_mask[:, :3] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITS = ['train', 'val', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2vec(a_list):\n",
    "    if type(a_list) == np.ndarray:\n",
    "        return a_list\n",
    "\n",
    "    elif type(a_list) == list:\n",
    "        if all([type(item) == list for item in a_list]):\n",
    "            a_list = [item for sublist in a_list for item in sublist]\n",
    "\n",
    "        if all([type(item) == str for item in a_list]):\n",
    "            a_list = np.asarray(a_list)\n",
    "        \n",
    "        if all([type(item) == np.ndarray for item in a_list]):\n",
    "            a_list = np.concatenate(a_list)\n",
    "        \n",
    "        return a_list\n",
    "    else:\n",
    "        raise TypeError('expected list or numpy array')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter training set, keeping only samples where target appears within mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = list2vec(data_gz['x_train'])\n",
    "y_train = list2vec(data_gz['y_train'])\n",
    "set_size_vec_train = list2vec(data_gz['set_size_vec_train'])\n",
    "\n",
    "splits_new = {\n",
    "    'train': {\n",
    "        'x': [],\n",
    "        'y': [],\n",
    "        'set_size_vec': [],\n",
    "    }\n",
    "}\n",
    "\n",
    "for fname, target_present, set_size in zip(x_train, y_train, set_size_vec_train):\n",
    "    if 'present' in fname:\n",
    "        fname_name = Path(fname).name\n",
    "        char_grid = fname_grid_map[fname_name]\n",
    "        if np.any(np.logical_and(char_grid == 't', train_mask)):\n",
    "            splits_new['train']['x'].append(fname)\n",
    "            splits_new['train']['set_size_vec'].append(set_size)\n",
    "            splits_new['train']['y'].append(target_present)\n",
    "    elif 'absent' in fname:\n",
    "        splits_new['train']['x'].append(fname)\n",
    "        splits_new['train']['set_size_vec'].append(set_size)\n",
    "        splits_new['train']['y'].append(target_present)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure equal number of samples for target present and absent conditions for each set size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_size_vec = np.asarray(splits_new['train']['set_size_vec'])\n",
    "set_sizes = np.unique(set_size_vec)\n",
    "y_train_new_arr = np.asarray(splits_new['train']['y'])\n",
    "keep_inds = []\n",
    "\n",
    "for set_size in set_sizes:\n",
    "    inds_this_set_size_target_present = np.nonzero(\n",
    "        np.logical_and(set_size_vec == set_size, y_train_new_arr == 1)\n",
    "    )[0]\n",
    "    inds_this_set_size_target_absent = np.nonzero(\n",
    "        np.logical_and(set_size_vec == set_size, y_train_new_arr == 0)\n",
    "    )[0]\n",
    "    num_present = inds_this_set_size_target_present.shape[0]\n",
    "    num_absent = inds_this_set_size_target_absent.shape[0]\n",
    "    if num_present < num_absent:\n",
    "        inds_this_set_size_target_absent = inds_this_set_size_target_absent[:num_present]\n",
    "    elif num_present > num_absent:\n",
    "        inds_this_set_size_target_present = inds_this_set_size_target_present[:num_absent]\n",
    "    else:\n",
    "        pass\n",
    "    keep_inds.extend(inds_this_set_size_target_absent.tolist())\n",
    "    keep_inds.extend(inds_this_set_size_target_present.tolist())\n",
    "\n",
    "keep_inds = np.asarray(keep_inds)\n",
    "for name in ['x', 'set_size_vec', 'y']:\n",
    "    as_arr = np.asarray(splits_new['train'][name])\n",
    "    splits_new['train'][name] = as_arr[keep_inds].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62382"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits_new['train']['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_gz['shard_train']:\n",
    "    shard_size = data_gz['shard_size']\n",
    "    # get floor to figure out num samples per shard for each set size,\n",
    "    # and then we'll throw any leftovers into the last (num_shards + 1) shard\n",
    "    num_shards = int(np.floor(len(splits_new['train']['x']) / shard_size))\n",
    "    set_sizes, set_size_samples_per_shard = np.unique(data_gz['set_size_vec_train'][0], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 382,  832, 1694, 3492])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_size_samples_per_shard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray(splits_new['train']['x'])\n",
    "y_train = np.asarray(splits_new['train']['y'])\n",
    "set_size_vec_train = np.asarray(splits_new['train']['set_size_vec'])\n",
    "for_sharding = {int(set_size): {} for set_size in\n",
    "                set_sizes}  # will add 'present' and 'absent' keys in next loop below\n",
    "for set_size in set_sizes:\n",
    "    set_size_inds = np.nonzero(set_size_vec_train == set_size)[0]\n",
    "    set_size_present_inds = np.nonzero(y_train[set_size_inds] == 1)[0]\n",
    "    set_size_absent_inds = np.nonzero(y_train[set_size_inds] == 1)[0]\n",
    "    for_sharding[int(set_size)]['present'] = x_train[set_size_present_inds].tolist()\n",
    "    for_sharding[int(set_size)]['absent'] = x_train[set_size_absent_inds].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_size, num_samples in zip(set_sizes, set_size_samples_per_shard):\n",
    "    is_odd = num_samples % 2\n",
    "    if is_odd:\n",
    "        coin_flip = np.random.choice([0, 1])\n",
    "        if coin_flip:\n",
    "            n_present = int(np.ceil(num_samples / 2))\n",
    "            n_absent = num_samples - n_present\n",
    "        else:\n",
    "            n_absent = int(np.ceil(num_samples / 2))\n",
    "            n_present = num_samples - n_absent\n",
    "    else:\n",
    "        n_present = n_absent = int(num_samples / 2)\n",
    "    set_size = int(set_size)\n",
    "    total_present = len(for_sharding[set_size]['present'])\n",
    "    for_sharding[set_size]['present'] = [\n",
    "        for_sharding[set_size]['present'][i:i + n_present] for i in range(0, total_present, n_present)]\n",
    "    total_absent = len(for_sharding[set_size]['absent'])\n",
    "    for_sharding[set_size]['absent'] = [\n",
    "        for_sharding[set_size]['absent'][i:i + n_absent] for i in range(0, total_absent, n_absent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sharded = []\n",
    "y_sharded = []\n",
    "set_size_sharded = []\n",
    "num_shards_now = set(\n",
    "    len(for_sharding[set_size][target_cond])\n",
    "    for set_size in set_sizes\n",
    "    for target_cond in ['present', 'absent']\n",
    ")\n",
    "if len(num_shards_now) != 1:\n",
    "    raise ValueError(f'inconsistent number of shards: {num_shards_now}')\n",
    "else:\n",
    "    num_shards_now = num_shards_now.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for shard_ind in range(num_shards_now):\n",
    "    x_shard = []\n",
    "    y_shard = []\n",
    "    set_size_shard = []\n",
    "    for set_size in set_sizes:\n",
    "        set_size = int(set_size)\n",
    "        x_present = for_sharding[set_size]['present'][shard_ind]\n",
    "        x_shard.extend(x_present)\n",
    "        y_shard.extend([1 for el in x_present])\n",
    "        set_size_shard.extend([set_size for el in x_present])\n",
    "\n",
    "        x_absent = for_sharding[set_size]['absent'][shard_ind]\n",
    "        x_shard.extend(x_absent)\n",
    "        y_shard.extend([0 for el in x_absent])\n",
    "        set_size_shard.extend([set_size for el in x_absent])\n",
    "\n",
    "    y_shard = np.asarray(y_shard)\n",
    "    set_size_shard = np.asarray(set_size_shard)\n",
    "    x_sharded.append(x_shard)\n",
    "    y_sharded.append(y_shard)\n",
    "    set_size_sharded.append(set_size_shard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6400"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    splits_new['train']['x'] = x_sharded\n",
    "    splits_new['train']['y'] = y_sharded\n",
    "    splits_new['train']['set_size_vec'] = set_size_sharded\n",
    "else:  # if shard_train is not True\n",
    "    # keep x as a list but\n",
    "    splits_new['train']['y'] = np.asarray(splits_new['train']['y'])\n",
    "    splits_new['train']['set_size_vec'] = np.asarray(splits_new['train']['set_size_vec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(for_sharding[2]['present'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = {}\n",
    "\n",
    "for split in SPLITS:\n",
    "        if split == 'train':\n",
    "            for name, a_list in splits_new['train'].items():\n",
    "                out_dict[f'{name}_{split}'] = splits_new[split][name]\n",
    "        elif split == 'val' or split == 'test':\n",
    "            for name in ['x', 'y', 'set_size_vec']:\n",
    "                out_dict[f'{name}_{split}'] = data_gz[f'{name}_{split}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALSO_ADD = ['set_sizes_by_stim_type', 'shard_train', 'shard_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for other_key in ALSO_ADD:\n",
    "    out_dict[other_key] = data_gz[other_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_DATAGZ_NAME = '/home/bart/Documents/repos/L2M/visual-search-nets/data/expt_13/data_prepd_for_nets/alexnet_train_test_target_split_RVvGV_data.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/bart/Documents/repos/L2M/visual-search-nets/data/expt_13/data_prepd_for_nets/alexnet_train_test_target_split_RVvGV_data.gz']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(out_dict, NEW_DATAGZ_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
