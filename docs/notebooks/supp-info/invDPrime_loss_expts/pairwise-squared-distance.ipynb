{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "import searchnets\n",
    "\n",
    "import mnist\n",
    "import mnist_contrib\n",
    "import fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/train-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "TRAIN_IMAGES = './mnist/train-images-idx3-ubyte.gz'\n",
    "\n",
    "with gfile.Open(TRAIN_IMAGES, 'rb') as f:\n",
    "    x_train = mnist_contrib.extract_images(f)\n",
    "    \n",
    "TRAIN_LABELS = './mnist/train-labels-idx1-ubyte.gz'\n",
    "with gfile.Open(TRAIN_LABELS, 'rb') as f:\n",
    "    y_train = mnist_contrib.extract_labels(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_01 = np.copy(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds_0 = np.where(np.isin(y_train, np.asarray([0,1,2,3,4])))[0]\n",
    "y_train_01[inds_0] = 0\n",
    "inds_1 = np.where(np.isin(y_train, np.asarray([5,6,7,8,9])))[0]\n",
    "y_train_01[inds_1] = 1\n",
    "\n",
    "x_train_01 = np.pad(x_train, ((0,0), (2, 2), (2,2), (0, 0)), mode='constant')  # so size is ( n x 32 x 32 x 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stoled from https://github.com/sujaybabruwad/LeNet-in-Tensorflow/blob/master/LeNet-Lab.ipynb\n",
    "\n",
    "class LeNet:\n",
    "    def __init__(self):    \n",
    "        self.mu = 0\n",
    "        self.sigma = 0.1\n",
    "        self.layer_depth = {\n",
    "            'layer_1' : 6,\n",
    "            'layer_2' : 16,\n",
    "            'layer_3' : 120,\n",
    "            'layer_f1' : 84\n",
    "        }\n",
    "        self.fc2 = None\n",
    "        self.logits = None\n",
    "\n",
    "    def build(self, x, n_classes=10):\n",
    "        # Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "        conv1_w = tf.Variable(tf.truncated_normal(shape = [5,5,1,6], mean = self.mu, stddev = self.sigma))\n",
    "        conv1_b = tf.Variable(tf.zeros(6))\n",
    "        conv1 = tf.nn.conv2d(x, conv1_w, strides = [1,1,1,1], padding = 'VALID') + conv1_b \n",
    "        conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "        # Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "        pool_1 = tf.nn.max_pool(conv1,ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "\n",
    "        # Layer 2: Convolutional. Output = 10x10x16.\n",
    "        conv2_w = tf.Variable(tf.truncated_normal(shape = [5,5,6,16], mean = self.mu, stddev = self.sigma))\n",
    "        conv2_b = tf.Variable(tf.zeros(16))\n",
    "        conv2 = tf.nn.conv2d(pool_1, conv2_w, strides = [1,1,1,1], padding = 'VALID') + conv2_b\n",
    "        conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "        # Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "        pool_2 = tf.nn.max_pool(conv2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID') \n",
    "\n",
    "        # Flatten. Input = 5x5x16. Output = 400.\n",
    "        fc1 = tf.layers.flatten(pool_2)\n",
    "\n",
    "        # Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "        fc1_w = tf.Variable(tf.truncated_normal(shape = (400,120), mean = self.mu, stddev = self.sigma))\n",
    "        fc1_b = tf.Variable(tf.zeros(120))\n",
    "        fc1 = tf.matmul(fc1,fc1_w) + fc1_b\n",
    "        fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "        # Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "        fc2_w = tf.Variable(tf.truncated_normal(shape = (120,84), mean = self.mu, stddev = self.sigma))\n",
    "        fc2_b = tf.Variable(tf.zeros(84))\n",
    "        fc2 = tf.matmul(fc1,fc2_w) + fc2_b\n",
    "        fc2 = tf.nn.relu(fc2)\n",
    "        self.fc2 = fc2\n",
    "\n",
    "        # Layer 5: Fully Connected. Input = 84. Output = n_classes.\n",
    "        fc3_w = tf.Variable(tf.truncated_normal(shape = (84, n_classes), mean = self.mu , stddev = self.sigma))\n",
    "        fc3_b = tf.Variable(tf.zeros(n_classes))\n",
    "        logits = tf.matmul(fc2, fc3_w) + fc3_b\n",
    "        self.logits = logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = [None] + list(x_train_01.shape[1:])\n",
    "\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/bart/anaconda3/envs/searchnets-tf1.13/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-6-1c3aee25bdc6>:36: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-79c577c0693a>:11: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lenet = LeNet()\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=input_shape, name='X')\n",
    "    y = tf.placeholder(tf.int32, shape=[None], name='Y')\n",
    "    y_onehot = tf.one_hot(y, depth=n_classes)\n",
    "\n",
    "    lenet.build(x=x, n_classes=2)\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_onehot, logits=lenet.logits), name='loss')\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate, name='SGD-op').minimize(loss)\n",
    "    correct_prediction = tf.equal(tf.argmax(lenet.logits, 1), tf.argmax(y_onehot, 1), name='correct_pred')\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize(x, y):\n",
    "    \"\"\" Randomizes the order of data samples and their corresponding labels\"\"\"\n",
    "    permutation = np.random.permutation(y.shape[0])\n",
    "    shuffled_x = x[permutation, :]\n",
    "    shuffled_y = y[permutation]\n",
    "    return shuffled_x, shuffled_y\n",
    "\n",
    "def get_next_batch(x, y, start, end):\n",
    "    x_batch = x[start:end]\n",
    "    y_batch = y[start:end]\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val(x_train, y_train, val_size):\n",
    "    x_val = x_train[-val_size:]\n",
    "    y_val = y_train[-val_size:]\n",
    "    x_train = x_train[:-val_size]\n",
    "    y_train = y_train[:-val_size]\n",
    "    return x_train, y_train, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_01, y_train_01, x_val_01, y_val_01 = get_val(x_train_01, y_train_01, val_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "iter   0:\t Loss=9.39,\tTraining Accuracy=37.0%\n",
      "iter 100:\t Loss=2.45,\tTraining Accuracy=64.0%\n",
      "iter 200:\t Loss=1.14,\tTraining Accuracy=76.0%\n",
      "iter 300:\t Loss=0.73,\tTraining Accuracy=81.0%\n",
      "iter 400:\t Loss=0.47,\tTraining Accuracy=82.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 1, validation loss: 0.68, validation accuracy: 81.2%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 2\n",
      "iter   0:\t Loss=0.48,\tTraining Accuracy=86.0%\n",
      "iter 100:\t Loss=0.70,\tTraining Accuracy=77.0%\n",
      "iter 200:\t Loss=0.39,\tTraining Accuracy=91.0%\n",
      "iter 300:\t Loss=0.43,\tTraining Accuracy=80.0%\n",
      "iter 400:\t Loss=0.33,\tTraining Accuracy=88.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 2, validation loss: 0.51, validation accuracy: 83.6%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 3\n",
      "iter   0:\t Loss=0.41,\tTraining Accuracy=82.0%\n",
      "iter 100:\t Loss=0.42,\tTraining Accuracy=82.0%\n",
      "iter 200:\t Loss=0.44,\tTraining Accuracy=84.0%\n",
      "iter 300:\t Loss=0.55,\tTraining Accuracy=84.0%\n",
      "iter 400:\t Loss=0.34,\tTraining Accuracy=84.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 3, validation loss: 0.42, validation accuracy: 86.1%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 4\n",
      "iter   0:\t Loss=0.57,\tTraining Accuracy=82.0%\n",
      "iter 100:\t Loss=0.31,\tTraining Accuracy=86.0%\n",
      "iter 200:\t Loss=0.34,\tTraining Accuracy=85.0%\n",
      "iter 300:\t Loss=0.24,\tTraining Accuracy=93.0%\n",
      "iter 400:\t Loss=0.26,\tTraining Accuracy=92.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 4, validation loss: 0.36, validation accuracy: 87.5%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 5\n",
      "iter   0:\t Loss=0.47,\tTraining Accuracy=87.0%\n",
      "iter 100:\t Loss=0.44,\tTraining Accuracy=84.0%\n",
      "iter 200:\t Loss=0.27,\tTraining Accuracy=87.0%\n",
      "iter 300:\t Loss=0.45,\tTraining Accuracy=81.0%\n",
      "iter 400:\t Loss=0.33,\tTraining Accuracy=91.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 5, validation loss: 0.34, validation accuracy: 87.4%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 6\n",
      "iter   0:\t Loss=0.34,\tTraining Accuracy=91.0%\n",
      "iter 100:\t Loss=0.39,\tTraining Accuracy=85.0%\n",
      "iter 200:\t Loss=0.26,\tTraining Accuracy=89.0%\n",
      "iter 300:\t Loss=0.27,\tTraining Accuracy=91.0%\n",
      "iter 400:\t Loss=0.20,\tTraining Accuracy=93.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 6, validation loss: 0.31, validation accuracy: 89.0%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 7\n",
      "iter   0:\t Loss=0.26,\tTraining Accuracy=90.0%\n",
      "iter 100:\t Loss=0.18,\tTraining Accuracy=93.0%\n",
      "iter 200:\t Loss=0.18,\tTraining Accuracy=96.0%\n",
      "iter 300:\t Loss=0.33,\tTraining Accuracy=90.0%\n",
      "iter 400:\t Loss=0.24,\tTraining Accuracy=91.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 7, validation loss: 0.30, validation accuracy: 88.8%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 8\n",
      "iter   0:\t Loss=0.29,\tTraining Accuracy=87.0%\n",
      "iter 100:\t Loss=0.21,\tTraining Accuracy=92.0%\n",
      "iter 200:\t Loss=0.18,\tTraining Accuracy=94.0%\n",
      "iter 300:\t Loss=0.17,\tTraining Accuracy=94.0%\n",
      "iter 400:\t Loss=0.30,\tTraining Accuracy=89.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 8, validation loss: 0.28, validation accuracy: 90.0%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 9\n",
      "iter   0:\t Loss=0.20,\tTraining Accuracy=91.0%\n",
      "iter 100:\t Loss=0.17,\tTraining Accuracy=92.0%\n",
      "iter 200:\t Loss=0.22,\tTraining Accuracy=93.0%\n",
      "iter 300:\t Loss=0.27,\tTraining Accuracy=88.0%\n",
      "iter 400:\t Loss=0.25,\tTraining Accuracy=92.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 9, validation loss: 0.27, validation accuracy: 90.5%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 10\n",
      "iter   0:\t Loss=0.16,\tTraining Accuracy=92.0%\n",
      "iter 100:\t Loss=0.18,\tTraining Accuracy=92.0%\n",
      "iter 200:\t Loss=0.20,\tTraining Accuracy=95.0%\n",
      "iter 300:\t Loss=0.22,\tTraining Accuracy=91.0%\n",
      "iter 400:\t Loss=0.35,\tTraining Accuracy=85.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 10, validation loss: 0.26, validation accuracy: 90.6%\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session(graph=graph)\n",
    "with sess:\n",
    "    sess.run(init)\n",
    "    global_step = 0\n",
    "    # Number of training iterations in each epoch\n",
    "    num_tr_iter = int(len(y_train_01) / batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        print('Training epoch: {}'.format(epoch + 1))\n",
    "        x_tr_epoch, y_tr_epoch = randomize(x_train_01, y_train_01)\n",
    "        for iteration in range(num_tr_iter):\n",
    "            global_step += 1\n",
    "            start = iteration * batch_size\n",
    "            end = (iteration + 1) * batch_size\n",
    "            x_batch, y_batch = get_next_batch(x_tr_epoch, y_tr_epoch, start, end)\n",
    "\n",
    "            # Run optimization op (backprop)\n",
    "            feed_dict_batch = {x: x_batch, y: y_batch}\n",
    "            sess.run(optimizer, feed_dict=feed_dict_batch)\n",
    "\n",
    "            if iteration % display_freq == 0:\n",
    "                # Calculate and display the batch loss and accuracy\n",
    "                loss_batch, acc_batch = sess.run([loss, accuracy],\n",
    "                                                 feed_dict=feed_dict_batch)\n",
    "\n",
    "                print(\"iter {0:3d}:\\t Loss={1:.2f},\\tTraining Accuracy={2:.01%}\".\n",
    "                      format(iteration, loss_batch, acc_batch))\n",
    "\n",
    "        # Run validation after every epoch\n",
    "        feed_dict_valid = {x: x_val_01[:1000], y: y_val_01[:1000]}\n",
    "        loss_valid, acc_valid = sess.run([loss, accuracy], feed_dict=feed_dict_valid)\n",
    "        print('---------------------------------------------------------')\n",
    "        print(\"Epoch: {0}, validation loss: {1:.2f}, validation accuracy: {2:.01%}\".\n",
    "              format(epoch + 1, loss_valid, acc_valid))\n",
    "        print('---------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_squared_distance(A):\n",
    "    \"\"\"compute pairwise squared distances between a batch of row vectors A.\n",
    "    Returns distance matrix D, where:\n",
    "        D[i,j] = (a[i]-a[j])(a[i]-a[j])'\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    A : tensorflow.Tensor\n",
    "        of rank 2, where each row a[i] is a feature vector output from a batch of input samples\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    D : tensorflow.Tensor\n",
    "        distance matrix, where D[i, j] is the pairwise squared distance between a[i] and a[j]\n",
    "\n",
    "    adapted from https://stackoverflow.com/questions/37009647/compute-pairwise-distance-in-a-batch-without-replicating-tensor-in-tensorflow\n",
    "    \"\"\"\n",
    "    # r[i] is squared norm of ith row of the original matrix\n",
    "    r = tf.reduce_sum(A*A, 1)\n",
    "\n",
    "    # turn r into column vector\n",
    "    r = tf.reshape(r, [-1, 1])\n",
    "    D = r - 2*tf.matmul(A, tf.transpose(A)) + tf.transpose(r)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_dist(A, B=None):\n",
    "    if B is None:\n",
    "        B = A\n",
    "\n",
    "    row_norms_A = tf.reduce_sum(tf.square(A), axis=1)\n",
    "    row_norms_A = tf.reshape(row_norms_A, [-1, 1])  # Column vector.\n",
    "\n",
    "    row_norms_B = tf.reduce_sum(tf.square(B), axis=1)\n",
    "    row_norms_B = tf.reshape(row_norms_B, [1, -1])  # Row vector.\n",
    "\n",
    "    return row_norms_A - 2 * tf.matmul(A, tf.transpose(B)) + row_norms_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclid_dist(A, B=None):\n",
    "    if B is None:\n",
    "        B = A\n",
    "    D = tf.sqrt(\n",
    "        tf.abs(squared_dist(A, B) + 1e-6)\n",
    "    )\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bart/anaconda3/envs/searchnets-tf1.13/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "lenet = LeNet()\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=input_shape, name='X')\n",
    "    y = tf.placeholder(tf.int32, shape=[None], name='Y')\n",
    "    y_onehot = tf.one_hot(y, depth=n_classes)\n",
    "\n",
    "    lenet.build(x=x, n_classes=2)\n",
    "    \n",
    "    ce_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_onehot, logits=lenet.logits), name='loss')\n",
    "\n",
    "    # t = target, d = distractor\n",
    "    t_inds = tf.where(tf.math.equal(y, 1))\n",
    "    t_vecs = tf.gather(lenet.fc2, t_inds)\n",
    "    t_vecs = tf.squeeze(t_vecs)\n",
    "    t_distances = euclid_dist(t_vecs)\n",
    "    mu_t_dist = tf.reduce_mean(t_distances)\n",
    "\n",
    "    d_inds = tf.where(tf.math.equal(y, 0))\n",
    "    d_vecs = tf.gather(lenet.fc2, d_inds)\n",
    "    d_vecs = tf.squeeze(d_vecs)\n",
    "    d_distances = euclid_dist(d_vecs)\n",
    "    mu_d_dist = tf.reduce_mean(d_distances)\n",
    "    \n",
    "    t_d_distances = euclid_dist(t_vecs, d_vecs)\n",
    "    mu_t_d_distances = tf.reduce_mean(t_d_distances)\n",
    "    \n",
    "    distance_loss = mu_t_dist + mu_d_dist + (1 / mu_t_d_distances)\n",
    "\n",
    "    loss = ce_loss + distance_loss\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate, name='SGD-op').minimize(loss)\n",
    "    preds = tf.argmax(lenet.logits, 1)\n",
    "    correct_prediction = tf.equal(preds, tf.argmax(y_onehot, 1), name='correct_pred')\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "epochs = 10             # Total number of training epochs\n",
    "batch_size = 100        # Training batch size\n",
    "display_freq = 100      # Frequency of displaying the training results\n",
    "learning_rate = 0.001   # The optimization initial learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "iter   0:\t Dist. Loss=316.84,\tLoss=330.39,\tTraining Accuracy=29.0%\n",
      "iter 100:\t Dist. Loss=2.83,\tLoss=3.52,\tTraining Accuracy=48.0%\n",
      "iter 200:\t Dist. Loss=2.83,\tLoss=3.52,\tTraining Accuracy=50.0%\n",
      "iter 300:\t Dist. Loss=2.83,\tLoss=3.52,\tTraining Accuracy=46.0%\n",
      "iter 400:\t Dist. Loss=2.80,\tLoss=3.48,\tTraining Accuracy=56.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 1, validation loss: 3.52, validation accuracy: 47.1%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 2\n",
      "iter   0:\t Dist. Loss=2.80,\tLoss=3.49,\tTraining Accuracy=54.0%\n",
      "iter 100:\t Dist. Loss=2.80,\tLoss=3.49,\tTraining Accuracy=49.0%\n",
      "iter 200:\t Dist. Loss=2.76,\tLoss=3.45,\tTraining Accuracy=50.0%\n",
      "iter 300:\t Dist. Loss=2.83,\tLoss=3.52,\tTraining Accuracy=49.0%\n",
      "iter 400:\t Dist. Loss=2.73,\tLoss=3.42,\tTraining Accuracy=48.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 2, validation loss: 3.50, validation accuracy: 45.5%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 3\n",
      "iter   0:\t Dist. Loss=2.77,\tLoss=3.46,\tTraining Accuracy=54.0%\n",
      "iter 100:\t Dist. Loss=2.71,\tLoss=3.40,\tTraining Accuracy=48.0%\n",
      "iter 200:\t Dist. Loss=2.79,\tLoss=3.48,\tTraining Accuracy=58.0%\n",
      "iter 300:\t Dist. Loss=2.77,\tLoss=3.46,\tTraining Accuracy=50.0%\n",
      "iter 400:\t Dist. Loss=2.73,\tLoss=3.43,\tTraining Accuracy=50.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 3, validation loss: 3.49, validation accuracy: 46.5%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 4\n",
      "iter   0:\t Dist. Loss=2.73,\tLoss=3.42,\tTraining Accuracy=53.0%\n",
      "iter 100:\t Dist. Loss=2.69,\tLoss=3.38,\tTraining Accuracy=49.0%\n",
      "iter 200:\t Dist. Loss=2.80,\tLoss=3.49,\tTraining Accuracy=50.0%\n",
      "iter 300:\t Dist. Loss=2.70,\tLoss=3.39,\tTraining Accuracy=47.0%\n",
      "iter 400:\t Dist. Loss=2.78,\tLoss=3.47,\tTraining Accuracy=52.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 4, validation loss: 3.48, validation accuracy: 48.8%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 5\n",
      "iter   0:\t Dist. Loss=2.76,\tLoss=3.46,\tTraining Accuracy=58.0%\n",
      "iter 100:\t Dist. Loss=2.75,\tLoss=3.44,\tTraining Accuracy=48.0%\n",
      "iter 200:\t Dist. Loss=2.73,\tLoss=3.42,\tTraining Accuracy=61.0%\n",
      "iter 300:\t Dist. Loss=2.71,\tLoss=3.40,\tTraining Accuracy=54.0%\n",
      "iter 400:\t Dist. Loss=2.79,\tLoss=3.48,\tTraining Accuracy=47.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 5, validation loss: 3.47, validation accuracy: 51.0%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 6\n",
      "iter   0:\t Dist. Loss=2.76,\tLoss=3.45,\tTraining Accuracy=55.0%\n",
      "iter 100:\t Dist. Loss=2.69,\tLoss=3.39,\tTraining Accuracy=41.0%\n",
      "iter 200:\t Dist. Loss=2.77,\tLoss=3.46,\tTraining Accuracy=50.0%\n",
      "iter 300:\t Dist. Loss=2.69,\tLoss=3.38,\tTraining Accuracy=52.0%\n",
      "iter 400:\t Dist. Loss=2.72,\tLoss=3.41,\tTraining Accuracy=52.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 6, validation loss: 3.48, validation accuracy: 52.5%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 7\n",
      "iter   0:\t Dist. Loss=2.64,\tLoss=3.33,\tTraining Accuracy=56.0%\n",
      "iter 100:\t Dist. Loss=2.69,\tLoss=3.38,\tTraining Accuracy=51.0%\n",
      "iter 200:\t Dist. Loss=2.76,\tLoss=3.45,\tTraining Accuracy=51.0%\n",
      "iter 300:\t Dist. Loss=2.76,\tLoss=3.45,\tTraining Accuracy=51.0%\n",
      "iter 400:\t Dist. Loss=2.71,\tLoss=3.40,\tTraining Accuracy=55.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 7, validation loss: 3.45, validation accuracy: 52.2%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 8\n",
      "iter   0:\t Dist. Loss=2.67,\tLoss=3.36,\tTraining Accuracy=46.0%\n",
      "iter 100:\t Dist. Loss=2.75,\tLoss=3.45,\tTraining Accuracy=48.0%\n",
      "iter 200:\t Dist. Loss=2.76,\tLoss=3.44,\tTraining Accuracy=54.0%\n",
      "iter 300:\t Dist. Loss=2.64,\tLoss=3.33,\tTraining Accuracy=41.0%\n",
      "iter 400:\t Dist. Loss=2.64,\tLoss=3.33,\tTraining Accuracy=50.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 8, validation loss: 3.45, validation accuracy: 51.9%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 9\n",
      "iter   0:\t Dist. Loss=2.73,\tLoss=3.42,\tTraining Accuracy=54.0%\n",
      "iter 100:\t Dist. Loss=2.73,\tLoss=3.42,\tTraining Accuracy=55.0%\n",
      "iter 200:\t Dist. Loss=2.74,\tLoss=3.43,\tTraining Accuracy=53.0%\n",
      "iter 300:\t Dist. Loss=2.71,\tLoss=3.40,\tTraining Accuracy=60.0%\n",
      "iter 400:\t Dist. Loss=2.69,\tLoss=3.37,\tTraining Accuracy=60.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 9, validation loss: 3.44, validation accuracy: 52.2%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 10\n",
      "iter   0:\t Dist. Loss=2.74,\tLoss=3.42,\tTraining Accuracy=52.0%\n",
      "iter 100:\t Dist. Loss=2.70,\tLoss=3.38,\tTraining Accuracy=60.0%\n",
      "iter 200:\t Dist. Loss=2.67,\tLoss=3.36,\tTraining Accuracy=52.0%\n",
      "iter 300:\t Dist. Loss=2.75,\tLoss=3.44,\tTraining Accuracy=46.0%\n",
      "iter 400:\t Dist. Loss=2.69,\tLoss=3.37,\tTraining Accuracy=56.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 10, validation loss: 3.44, validation accuracy: 52.3%\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session(graph=graph)\n",
    "with sess:\n",
    "    sess.run(init)\n",
    "    global_step = 0\n",
    "    # Number of training iterations in each epoch\n",
    "    num_tr_iter = int(len(y_train_01) / batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        print('Training epoch: {}'.format(epoch + 1))\n",
    "        x_tr_epoch, y_tr_epoch = randomize(x_train_01, y_train_01)\n",
    "        for iteration in range(num_tr_iter):\n",
    "            global_step += 1\n",
    "            start = iteration * batch_size\n",
    "            end = (iteration + 1) * batch_size\n",
    "            x_batch, y_batch = get_next_batch(x_tr_epoch, y_tr_epoch, start, end)\n",
    "\n",
    "            # Run optimization op (backprop)\n",
    "            feed_dict_batch = {x: x_batch, y: y_batch}\n",
    "            sess.run(optimizer, feed_dict=feed_dict_batch)\n",
    "\n",
    "            if iteration % display_freq == 0:\n",
    "                # Calculate and display the batch loss and accuracy\n",
    "                preds_b, correct_b, dist_loss_b, loss_batch, acc_batch = sess.run([preds, correct_prediction, \n",
    "                                                                                   distance_loss, loss, accuracy],\n",
    "                                                 feed_dict=feed_dict_batch)\n",
    "\n",
    "                print(\"iter {0:3d}:\\t Dist. Loss={1:.2f},\\tLoss={2:.2f},\\tTraining Accuracy={3:.01%}\".\n",
    "                      format(iteration, dist_loss_b, loss_batch, acc_batch))\n",
    "\n",
    "        # Run validation after every epoch\n",
    "        feed_dict_valid = {x: x_val_01[:1000], y: y_val_01[:1000]}\n",
    "        loss_valid, acc_valid = sess.run([loss, accuracy], feed_dict=feed_dict_valid)\n",
    "        print('---------------------------------------------------------')\n",
    "        print(\"Epoch: {0}, validation loss: {1:.2f}, validation accuracy: {2:.01%}\".\n",
    "              format(epoch + 1, loss_valid, acc_valid))\n",
    "        print('---------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not working, I think because the loss function is dominated by the distance values.\n",
    "\n",
    "Perhaps better to have the network learn two functions simultaneously, e.g. a score function and the classification.\n",
    "\n",
    "So literally do siamese training, just add a score head, back-prop through that.\n",
    "\n",
    "In fact, probably use triplet loss. See:\n",
    "https://omoindrot.github.io/triplet-loss\n",
    "https://github.com/omoindrot/tensorflow-triplet-loss\n",
    "But do it at the same time as cross-entropy, i.e. be able to do both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
