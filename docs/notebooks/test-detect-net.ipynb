{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pyprojroot\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import searchnets\n",
    "from searchnets import nets\n",
    "from searchnets.datasets import VOCDetection\n",
    "from searchnets.engine.abstract_trainer import AbstractTrainer\n",
    "from searchnets.transforms.util import get_transforms\n",
    "from searchnets.utils.general import make_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile(a, dim, n_tile):\n",
    "    dim_init_size = a.size(dim)\n",
    "    repeat_idx = [1] * a.dim()\n",
    "    repeat_idx[dim] = n_tile\n",
    "    a = a.repeat(*(repeat_idx))\n",
    "    order_index = torch.LongTensor(\n",
    "        np.concatenate([dim_init_size * np.arange(n_tile) + i for i in range(dim_init_size)])\n",
    "    )\n",
    "    return torch.index_select(a, dim, order_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vis_sys,\n",
    "                 num_classes,\n",
    "                 vis_sys_n_out,\n",
    "                 embedding_n_out=512):\n",
    "        super(DetectNet, self).__init__()\n",
    "        self.vis_sys = vis_sys\n",
    "        self.embedding = nn.Sequential(nn.Linear(in_features=num_classes,\n",
    "                                                 out_features=embedding_n_out),\n",
    "                                       nn.ReLU(inplace=True),\n",
    "                                       )\n",
    "        self.decoder = nn.Linear(in_features=vis_sys_n_out + embedding_n_out,\n",
    "                                 out_features=1)  # always 1, because it indicates target present or absent\n",
    "\n",
    "    def forward(self, img, query):\n",
    "        vis_out = self.vis_sys(img)\n",
    "        query_out = self.embedding(query)\n",
    "        out = self.decoder(\n",
    "            torch.cat((vis_out, query_out), dim=1)\n",
    "        )\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_TYPE = 'VOC'\n",
    "LOSS_FUNCS = ['CE-largest', 'CE-random', 'BCE']\n",
    "\n",
    "PAD_SIZE = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VSD_CONFIGS_ROOT = pyprojroot.here('./data/configs/VSD')\n",
    "VSD_CONFIG_INIS = sorted(VSD_CONFIGS_ROOT.glob('*.ini'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALEXNET_VSD_CONFIG_INIS = [vsd_config for vsd_config in VSD_CONFIG_INIS if 'alexnet' in str(vsd_config) and 'transfer' in str(vsd_config)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "configfile = ALEXNET_VSD_CONFIG_INIS[0]\n",
    "cfg = searchnets.config.parse_config(configfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = searchnets.config.parse_config(configfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /home/bart/Documents/data/voc/VOCtrainval_11-May-2012.tar\n"
     ]
    }
   ],
   "source": [
    "transform, target_transform = get_transforms(cfg.data.dataset_type, \n",
    "                                             loss_func=cfg.train.loss_func,\n",
    "                                             pad_size=cfg.data.pad_size)\n",
    "\n",
    "trainset = VOCDetection(root=cfg.data.root,\n",
    "                       csv_file=cfg.data.csv_file_out,\n",
    "                       image_set='trainval',\n",
    "                       split='train',\n",
    "                       download=True,\n",
    "                       transform=transform,\n",
    "                       target_transform=target_transform\n",
    "                       )\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=cfg.train.batch_size,\n",
    "                          shuffle=False, num_workers=cfg.train.num_workers,\n",
    "                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = searchnets.nets.vgg16.build(pretrained=True)\n",
    "vgg16.classifier = vgg16.classifier[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_batch = next(iter(train_loader))\n",
    "tmp_img = a_batch['img']\n",
    "tmp_out = vgg16(tmp_img)\n",
    "n_out = tmp_out.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DetectNet(vis_sys=vgg16,\n",
    "                  num_classes=20,\n",
    "                  vis_sys_n_out=n_out,\n",
    "                  embedding_n_out=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f'device: {device}')\n",
    "\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_learn_rate_params = list(model.vis_sys.classifier.parameters())\n",
    "new_learn_rate_params += list(model.embedding.parameters())\n",
    "new_learn_rate_params += list(model.decoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOMENTUM = 0.9\n",
    "\n",
    "optimizers = []\n",
    "optimizers.append(\n",
    "    torch.optim.SGD(new_learn_rate_params,\n",
    "                    lr=cfg.train.new_layer_learning_rate,\n",
    "                    momentum=MOMENTUM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_params = model.vis_sys.features.parameters()\n",
    "for params in feature_params:\n",
    "    params.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch 360 of 361, loss:   0.608: 100%|██████████| 361/361 [02:18<00:00,  2.96it/s]\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "half_batch_size = int(cfg.train.batch_size / 2)\n",
    "\n",
    "batch_total = int(np.ceil(len(trainset) / cfg.train.batch_size))\n",
    "batch_pbar = tqdm(train_loader)\n",
    "for i, batch in enumerate(batch_pbar):\n",
    "    img, query = batch['img'], batch['target']\n",
    "    img_tile = tile(img, 0, 20)\n",
    "    batch_size, n_classes = query.shape\n",
    "    target = query.flatten()\n",
    "    query_expanded = torch.cat(batch_size * [torch.diag(torch.ones(n_classes,))])\n",
    "    \n",
    "    # -- make half of batch be target present, half target absent --\n",
    "    all_target_present_inds = np.nonzero(target == 1)\n",
    "    target_present_to_use = torch.randperm(all_target_present_inds.shape[0])[:half_batch_size]\n",
    "    target_present_inds = all_target_present_inds[target_present_to_use]\n",
    "\n",
    "    n_absent = target_present_to_use.shape[0]  # might be less than half_batch_size\n",
    "    all_target_absent_inds = np.nonzero(target == 0)\n",
    "    target_absent_to_use = torch.randperm(all_target_absent_inds.shape[0])[:n_absent]\n",
    "    target_absent_inds = all_target_absent_inds[target_absent_to_use]\n",
    "    \n",
    "    batch_inds = torch.cat((target_present_inds, target_absent_inds)).flatten().sort()[0]\n",
    "    \n",
    "    target = target.unsqueeze(1)  # add back non-batch ind, so target matches output shape\n",
    "    \n",
    "    img, query, target = img_tile[batch_inds].to(device), query_expanded[batch_inds].to(device), target[batch_inds].to(device)\n",
    "\n",
    "    output = model(img, query)\n",
    "    loss = criterion(output, target)\n",
    "\n",
    "    for optimizer in optimizers:\n",
    "        optimizer.zero_grad()\n",
    "    loss.mean().backward()  # mean needed for multiple GPUs\n",
    "    for optimizer in optimizers:\n",
    "        optimizer.step()\n",
    "\n",
    "    batch_pbar.set_description(f'batch {i} of {batch_total}, loss: {loss: 7.3f}')\n",
    "    total_loss += loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8., device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_batch['target'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.tensortype"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.vis_sys.classifier.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
